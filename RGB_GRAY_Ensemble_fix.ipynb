{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e039b65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'pytorch'\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "427176e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 1\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3997a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 2\n",
    "CFG = {\n",
    "    'IMG_SIZE': 224,\n",
    "    'BATCH_SIZE': 64,\n",
    "    'EPOCHS': 100,\n",
    "    'LEARNING_RATE': 1e-4,\n",
    "    'PATIENCE': 10,\n",
    "    'SEED': 42\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9915c4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 3\n",
    "torch.manual_seed(CFG['SEED'])\n",
    "np.random.seed(CFG['SEED'])\n",
    "random.seed(CFG['SEED'])\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4265b9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 4\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, file_list, label_list, transform):\n",
    "        self.file_list = file_list\n",
    "        self.label_list = label_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.file_list[idx]).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        label = self.label_list[idx]\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0be28ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 5\n",
    "transform_rgb = transforms.Compose([\n",
    "    transforms.Resize((CFG['IMG_SIZE'], CFG['IMG_SIZE'])),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
    "])\n",
    "\n",
    "transform_gray = transforms.Compose([\n",
    "    transforms.Resize((CFG['IMG_SIZE'], CFG['IMG_SIZE'])),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18be14b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 6\n",
    "from glob import glob\n",
    "\n",
    "class_dirs = sorted(os.listdir('./train'))\n",
    "file_paths, labels = [], []\n",
    "for i, cls in enumerate(class_dirs):\n",
    "    files = glob(f'./train/{cls}/*.jpg')\n",
    "    file_paths.extend(files)\n",
    "    labels.extend([i] * len(files))\n",
    "\n",
    "train_x, val_x, train_y, val_y = train_test_split(file_paths, labels, test_size=0.2, stratify=labels, random_state=CFG['SEED'])\n",
    "\n",
    "train_dataset_rgb = CustomImageDataset(train_x, train_y, transform_rgb)\n",
    "val_dataset_rgb = CustomImageDataset(val_x, val_y, transform_rgb)\n",
    "train_dataset_gray = CustomImageDataset(train_x, train_y, transform_gray)\n",
    "val_dataset_gray = CustomImageDataset(val_x, val_y, transform_gray)\n",
    "\n",
    "train_loader_rgb = DataLoader(train_dataset_rgb, batch_size=CFG['BATCH_SIZE'], shuffle=True)\n",
    "val_loader_rgb = DataLoader(val_dataset_rgb, batch_size=CFG['BATCH_SIZE'], shuffle=False)\n",
    "train_loader_gray = DataLoader(train_dataset_gray, batch_size=CFG['BATCH_SIZE'], shuffle=True)\n",
    "val_loader_gray = DataLoader(val_dataset_gray, batch_size=CFG['BATCH_SIZE'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ec4bf30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet201_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet201_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\user\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\user\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\timm\\models\\_factory.py:126: UserWarning: Mapping deprecated model name xception to current legacy_xception.\n",
      "  model = create_fn(\n",
      "c:\\Users\\user\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "import timm\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# 클래스 수 자동 계산\n",
    "num_classes = len(class_dirs)\n",
    "\n",
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(EnsembleModel, self).__init__()\n",
    "\n",
    "        self.model1 = models.densenet201(pretrained=True)\n",
    "        self.model1.classifier = nn.Identity()\n",
    "        self.dim1 = 1920\n",
    "\n",
    "        self.model2 = models.vgg16(pretrained=True)\n",
    "        self.model2.classifier = nn.Sequential(*list(self.model2.classifier.children())[:-1])\n",
    "        self.dim2 = 4096\n",
    "\n",
    "        self.model3 = timm.create_model('xception', pretrained=True, num_classes=0)\n",
    "        self.dim3 = 2048\n",
    "\n",
    "        self.classifier = nn.Linear(self.dim1 + self.dim2 + self.dim3, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.model1(x)\n",
    "        x2 = self.model2(x)\n",
    "        x3 = self.model3(x)\n",
    "        x_cat = torch.cat((x1, x2, x3), dim=1)\n",
    "        out = self.classifier(x_cat)\n",
    "        return out\n",
    "\n",
    "# 모델 초기화 (클래스 수 인자로 전달)\n",
    "model_rgb = EnsembleModel(num_classes).to(device)\n",
    "model_gray = EnsembleModel(num_classes).to(device)\n",
    "\n",
    "# 옵티마이저와 손실 함수 정의\n",
    "optimizer_rgb = optim.Adam(model_rgb.parameters(), lr=CFG['LEARNING_RATE'])\n",
    "optimizer_gray = optim.Adam(model_gray.parameters(), lr=CFG['LEARNING_RATE'])\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "264f880f",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m model_gray\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      9\u001b[0m train_loss_rgb, train_loss_gray \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (x_rgb, y), (x_gray, _) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(train_loader_rgb, train_loader_gray):\n\u001b[0;32m     12\u001b[0m     x_rgb, y \u001b[38;5;241m=\u001b[39m x_rgb\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     13\u001b[0m     x_gray \u001b[38;5;241m=\u001b[39m x_gray\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\user\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\user\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n",
      "File \u001b[1;32mc:\\Users\\user\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:620\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter._next_index\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    619\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_index\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 620\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sampler_iter\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\torch\\utils\\data\\sampler.py:283\u001b[0m, in \u001b[0;36mBatchSampler.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    281\u001b[0m batch \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[0;32m    282\u001b[0m idx_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 283\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampler:\n\u001b[0;32m    284\u001b[0m     batch[idx_in_batch] \u001b[38;5;241m=\u001b[39m idx\n\u001b[0;32m    285\u001b[0m     idx_in_batch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\user\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\torch\\utils\\data\\sampler.py:165\u001b[0m, in \u001b[0;36mRandomSampler.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n):\n\u001b[1;32m--> 165\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mint\u001b[39m, \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandperm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mint\u001b[39m, torch\u001b[38;5;241m.\u001b[39mrandperm(n, generator\u001b[38;5;241m=\u001b[39mgenerator)[:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m%\u001b[39m n]\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "# cell 8\n",
    "best_val_loss_rgb = float('inf')\n",
    "best_val_loss_gray = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(CFG['EPOCHS']):\n",
    "    model_rgb.train()\n",
    "    model_gray.train()\n",
    "    train_loss_rgb, train_loss_gray = 0, 0\n",
    "\n",
    "    for (x_rgb, y), (x_gray, _) in zip(train_loader_rgb, train_loader_gray):\n",
    "        x_rgb, y = x_rgb.to(device), y.to(device)\n",
    "        x_gray = x_gray.to(device)\n",
    "\n",
    "        optimizer_rgb.zero_grad()\n",
    "        output_rgb = model_rgb(x_rgb)\n",
    "        loss_rgb = criterion(output_rgb, y)\n",
    "        loss_rgb.backward()\n",
    "        optimizer_rgb.step()\n",
    "        train_loss_rgb += loss_rgb.item()\n",
    "\n",
    "        optimizer_gray.zero_grad()\n",
    "        output_gray = model_gray(x_gray)\n",
    "        loss_gray = criterion(output_gray, y)\n",
    "        loss_gray.backward()\n",
    "        optimizer_gray.step()\n",
    "        train_loss_gray += loss_gray.item()\n",
    "\n",
    "    model_rgb.eval()\n",
    "    model_gray.eval()\n",
    "    val_loss_rgb, val_loss_gray = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (x_rgb, y), (x_gray, _) in zip(val_loader_rgb, val_loader_gray):\n",
    "            x_rgb, y = x_rgb.to(device), y.to(device)\n",
    "            x_gray = x_gray.to(device)\n",
    "\n",
    "            output_rgb = model_rgb(x_rgb)\n",
    "            output_gray = model_gray(x_gray)\n",
    "            val_loss_rgb += criterion(output_rgb, y).item()\n",
    "            val_loss_gray += criterion(output_gray, y).item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: RGB Loss={val_loss_rgb:.4f} / GRAY Loss={val_loss_gray:.4f}\")\n",
    "\n",
    "    if val_loss_rgb < best_val_loss_rgb:\n",
    "        best_val_loss_rgb = val_loss_rgb\n",
    "        torch.save(model_rgb.state_dict(), 'best_rgb_model.pth')\n",
    "\n",
    "    if val_loss_gray < best_val_loss_gray:\n",
    "        best_val_loss_gray = val_loss_gray\n",
    "        torch.save(model_gray.state_dict(), 'best_gray_model.pth')\n",
    "\n",
    "    if val_loss_rgb >= best_val_loss_rgb and val_loss_gray >= best_val_loss_gray:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= CFG['PATIENCE']:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "    else:\n",
    "        patience_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6502986b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11 (pyenv)",
   "language": "python",
   "name": "pyenv310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
